{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71639194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('metaverse_transactions_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d255dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['transaction_type', 'location_region', 'purchase_pattern', 'age_group', 'anomaly']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb41a9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>location_region</th>\n",
       "      <th>ip_prefix</th>\n",
       "      <th>login_frequency</th>\n",
       "      <th>session_duration</th>\n",
       "      <th>purchase_pattern</th>\n",
       "      <th>age_group</th>\n",
       "      <th>risk_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>796.949206</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>192.000</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>172.000</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>778.197390</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>300.838358</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>172.000</td>\n",
       "      <td>8</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>775.569344</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>172.160</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>62.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78595</th>\n",
       "      <td>12</td>\n",
       "      <td>660.280373</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>172.000</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78596</th>\n",
       "      <td>16</td>\n",
       "      <td>310.273397</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>172.000</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78597</th>\n",
       "      <td>16</td>\n",
       "      <td>624.674332</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>192.000</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78598</th>\n",
       "      <td>4</td>\n",
       "      <td>401.391592</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78599</th>\n",
       "      <td>14</td>\n",
       "      <td>523.947956</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>172.000</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78600 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hour_of_day      amount  transaction_type  location_region  ip_prefix  \\\n",
       "0               12  796.949206                 4                2    192.000   \n",
       "1               19    0.010000                 1                4    172.000   \n",
       "2               16  778.197390                 1                1    192.168   \n",
       "3                9  300.838358                 4                4    172.000   \n",
       "4               14  775.569344                 2                0    172.160   \n",
       "...            ...         ...               ...              ...        ...   \n",
       "78595           12  660.280373                 4                0    172.000   \n",
       "78596           16  310.273397                 1                0    172.000   \n",
       "78597           16  624.674332                 1                0    192.000   \n",
       "78598            4  401.391592                 1                1    192.168   \n",
       "78599           14  523.947956                 4                3    172.000   \n",
       "\n",
       "       login_frequency  session_duration  purchase_pattern  age_group  \\\n",
       "0                    3                48                 0          0   \n",
       "1                    5                61                 0          0   \n",
       "2                    3                74                 0          0   \n",
       "3                    8               111                 1          2   \n",
       "4                    6               100                 1          2   \n",
       "...                ...               ...               ...        ...   \n",
       "78595                1                27                 2          1   \n",
       "78596                5                60                 0          0   \n",
       "78597                1                34                 2          1   \n",
       "78598                4                56                 0          0   \n",
       "78599                4                56                 0          0   \n",
       "\n",
       "       risk_score  \n",
       "0         18.7500  \n",
       "1         25.0000  \n",
       "2         31.2500  \n",
       "3         36.7500  \n",
       "4         62.5000  \n",
       "...           ...  \n",
       "78595     26.2500  \n",
       "78596     26.2500  \n",
       "78597     36.7500  \n",
       "78598     35.4375  \n",
       "78599     15.7500  \n",
       "\n",
       "[78600 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into features and target variable\n",
    "X = data.drop(columns=['anomaly', 'timestamp', 'sending_address', 'receiving_address'])\n",
    "y = data['anomaly']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04a8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "# Initialize and train the Isolation Forest model\n",
    "model = IsolationForest(contamination=0.01, random_state=42)  # Adjust contamination based on expected fraud rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec07252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(contamination=0.01, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(contamination=0.01, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(contamination=0.01, random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c6e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.92      0.14      5244\n",
      "           1       0.25      0.00      0.01     50646\n",
      "           2       0.00      0.00      0.00      6990\n",
      "\n",
      "    accuracy                           0.08     62880\n",
      "   macro avg       0.11      0.31      0.05     62880\n",
      "weighted avg       0.20      0.08      0.02     62880\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.92      0.14      1251\n",
      "           1       0.24      0.00      0.01     12848\n",
      "           2       0.00      0.00      0.00      1621\n",
      "\n",
      "    accuracy                           0.08     15720\n",
      "   macro avg       0.11      0.31      0.05     15720\n",
      "weighted avg       0.20      0.08      0.02     15720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict the anomaly labels\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "# Convert predicted labels to binary (1 for inliers, -1 for outliers)\n",
    "y_pred_train[y_pred_train == 1] = 0\n",
    "y_pred_train[y_pred_train == -1] = 1\n",
    "y_pred_test[y_pred_test == 1] = 0\n",
    "y_pred_test[y_pred_test == -1] = 1\n",
    "\n",
    "# Print classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce7ae2",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The classification reports provide insights into the performance of the fraud detection model on both the training and test datasets. Let's interpret the key metrics:\n",
    "\n",
    "Training Classification Report:\n",
    "Precision: Precision measures the proportion of correctly predicted instances among all instances predicted as a particular class.\n",
    "For class 0 (low-risk transactions), the precision is 0.08, indicating that among all instances predicted as low-risk, only 8% are actually low-risk transactions.\n",
    "For class 1 (moderate-risk transactions), the precision is 0.25, suggesting that among instances predicted as moderate-risk, 25% are actually moderate-risk transactions.\n",
    "For class 2 (high-risk transactions), the precision is 0, indicating that no instances were correctly predicted as high-risk transactions.\n",
    "Recall: Recall measures the proportion of correctly predicted instances of a particular class among all instances of that class.\n",
    "For class 0, the recall is 0.92, meaning that 92% of actual low-risk transactions were correctly classified.\n",
    "For class 1, the recall is 0.00, indicating that none of the actual moderate-risk transactions were correctly classified.\n",
    "For class 2, the recall is 0.00, meaning that none of the actual high-risk transactions were correctly classified.\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, providing a balanced measure between precision and recall.\n",
    "For class 0, the F1-score is 0.14, which is relatively low.\n",
    "For classes 1 and 2, the F1-scores are very low, indicating poor performance in correctly predicting these classes.\n",
    "Test Classification Report:\n",
    "The interpretation of the test classification report is similar to the training classification report but applied to the test dataset.\n",
    "\n",
    "Overall Interpretation:\n",
    "The model has high recall but very low precision for low-risk transactions, indicating that it correctly identifies most low-risk transactions but also misclassifies many other transactions as low-risk.\n",
    "For moderate-risk and high-risk transactions, the model's performance is poor, with low precision, recall, and F1-scores, indicating that it fails to accurately identify these transactions.\n",
    "The overall accuracy is low, suggesting that the model's predictions are not reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a001a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Model Building\n",
    "# Initialize and train the Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators based on your dataset and computational resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71aa86d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4555a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Results:\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "           1       1.00      1.00      1.00     50646\n",
      "           2       1.00      1.00      1.00      6990\n",
      "\n",
      "    accuracy                           1.00     62880\n",
      "   macro avg       1.00      1.00      1.00     62880\n",
      "weighted avg       1.00      1.00      1.00     62880\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1251\n",
      "           1       1.00      1.00      1.00     12848\n",
      "           2       1.00      1.00      1.00      1621\n",
      "\n",
      "    accuracy                           1.00     15720\n",
      "   macro avg       1.00      1.00      1.00     15720\n",
      "weighted avg       1.00      1.00      1.00     15720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the anomaly labels\n",
    "gb_y_pred_train = gb_model.predict(X_train)\n",
    "gb_y_pred_test = gb_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Gradient Boosting Classifier Results:\")\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train, gb_y_pred_train))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, gb_y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f319f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='accuracy', n_jobs= 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b2f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gradient Boosting Classifier with best parameters\n",
    "gb_model_best = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the best parameters\n",
    "gb_model_best.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7495291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "gb_y_pred_train_best = gb_model_best.predict(X_train)\n",
    "gb_y_pred_test_best = gb_model_best.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Gradient Boosting Classifier Results with Cross-validation and Hyperparameter Tuning:\")\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train, gb_y_pred_train_best))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, gb_y_pred_test_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6095ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Results with Cross-validation and Hyperparameter Tuning:\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "           1       1.00      1.00      1.00     50646\n",
      "           2       1.00      1.00      1.00      6990\n",
      "\n",
      "    accuracy                           1.00     62880\n",
      "   macro avg       1.00      1.00      1.00     62880\n",
      "weighted avg       1.00      1.00      1.00     62880\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1251\n",
      "           1       1.00      1.00      1.00     12848\n",
      "           2       1.00      1.00      1.00      1621\n",
      "\n",
      "    accuracy                           1.00     15720\n",
      "   macro avg       1.00      1.00      1.00     15720\n",
      "weighted avg       1.00      1.00      1.00     15720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the parameter distributions for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10)\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(gb_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Initialize Gradient Boosting Classifier with best parameters\n",
    "gb_model_best = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "gb_model_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "gb_y_pred_train_best = gb_model_best.predict(X_train)\n",
    "gb_y_pred_test_best = gb_model_best.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Gradient Boosting Classifier Results with Cross-validation and Hyperparameter Tuning:\")\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train, gb_y_pred_train_best))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, gb_y_pred_test_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b80a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/e1/4c/4685ccfae9806f561de716e32549190c1f533dde5bcadaf83bdf23972cf0/lightgbm-4.3.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.3.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\prudh\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\prudh\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n",
      "Downloading lightgbm-4.3.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.3 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.6/1.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32428822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408ab2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data head after dropping and converting:\n",
      "   hour_of_day      amount  transaction_type  location_region  ip_prefix  \\\n",
      "0           12  796.949206                 4                2    192.000   \n",
      "1           19    0.010000                 1                4    172.000   \n",
      "2           16  778.197390                 1                1    192.168   \n",
      "3            9  300.838358                 4                4    172.000   \n",
      "4           14  775.569344                 2                0    172.160   \n",
      "\n",
      "   login_frequency  session_duration  purchase_pattern  age_group  risk_score  \n",
      "0                3                48                 0          0       18.75  \n",
      "1                5                61                 0          0       25.00  \n",
      "2                3                74                 0          0       31.25  \n",
      "3                8               111                 1          2       36.75  \n",
      "4                6               100                 1          2       62.50  \n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['timestamp', 'sending_address', 'receiving_address', 'anomaly']\n",
    "data = data.drop(drop_columns, axis=1)\n",
    "\n",
    "object_columns = ['transaction_type', 'location_region', 'purchase_pattern', 'age_group']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in object_columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "print(\"Data head after dropping and converting:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fde87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('risk_score', axis=1)\n",
    "y = data['risk_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ccd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3b6074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest - MSE: 0.001721175167978746, MAE: 0.0012473361959368305\n",
      "LinearRegression - MSE: 216.2109402499176, MAE: 9.524466480567389\n",
      "SVR - MSE: 455.0748173291029, MAE: 14.43454112888812\n",
      "DecisionTree - MSE: 0.0013901081424936394, MAE: 0.0005693384224198866\n",
      "KNeighbors - MSE: 305.3305434792462, MAE: 11.676624204834607\n",
      "GradientBoosting - MSE: 0.5663464359049976, MAE: 0.5272205271915169\n",
      "AdaBoost - MSE: 28.666083580845108, MAE: 4.236790983512714\n",
      "Bagging - MSE: 0.001987783476463171, MAE: 0.0012402989821941248\n",
      "XGB - MSE: 0.09847261365965428, MAE: 0.02810204963344355\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 451\n",
      "[LightGBM] [Info] Number of data points in the train set: 62880, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 45.067413\n",
      "LightGBM - MSE: 62.76646409020396, MAE: 6.20240695574423\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'SVR': SVR(),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42), \n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'Bagging': BaggingRegressor(random_state=42),\n",
    "    'XGB': XGBRegressor(random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(learning_rate=0.01, n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"{name} - MSE: {mse}, MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a1e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results:\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "           1       1.00      1.00      1.00     50646\n",
      "           2       1.00      1.00      1.00      6990\n",
      "\n",
      "    accuracy                           1.00     62880\n",
      "   macro avg       1.00      1.00      1.00     62880\n",
      "weighted avg       1.00      1.00      1.00     62880\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1251\n",
      "           1       1.00      1.00      1.00     12848\n",
      "           2       1.00      1.00      1.00      1621\n",
      "\n",
      "    accuracy                           1.00     15720\n",
      "   macro avg       1.00      1.00      1.00     15720\n",
      "weighted avg       1.00      1.00      1.00     15720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "rf_y_pred_train = rf_model.predict(X_train)\n",
    "rf_y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train, rf_y_pred_train))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, rf_y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e159e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Results:\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5244\n",
      "           1       1.00      1.00      1.00     50646\n",
      "           2       1.00      1.00      1.00      6990\n",
      "\n",
      "    accuracy                           1.00     62880\n",
      "   macro avg       1.00      1.00      1.00     62880\n",
      "weighted avg       1.00      1.00      1.00     62880\n",
      "\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1251\n",
      "           1       1.00      1.00      1.00     12848\n",
      "           2       1.00      1.00      1.00      1621\n",
      "\n",
      "    accuracy                           1.00     15720\n",
      "   macro avg       1.00      1.00      1.00     15720\n",
      "weighted avg       1.00      1.00      1.00     15720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize base estimators\n",
    "rf_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_estimator = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "lr_estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Initialize Voting Classifier\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_estimator),\n",
    "    ('gradient_boosting', gb_estimator),\n",
    "    ('logistic_regression', lr_estimator)\n",
    "], voting='hard')  # 'hard' for majority voting\n",
    "\n",
    "# Train the Voting Classifier\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "voting_y_pred_train = voting_model.predict(X_train)\n",
    "voting_y_pred_test = voting_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Voting Classifier Results:\")\n",
    "print(\"\\nTraining Classification Report:\")\n",
    "print(classification_report(y_train, voting_y_pred_train))\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, voting_y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac4b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 1.0\n",
      "GradientBoostingClassifier 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9906488549618321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "for clf in (rf_estimator, gb_estimator, lr_estimator,voting_model):\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.__class__.__name__, clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587cc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
